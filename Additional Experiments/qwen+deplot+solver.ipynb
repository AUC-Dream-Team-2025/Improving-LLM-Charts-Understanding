{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYBZRumFmSgm",
        "outputId": "c5eb4619-266f-460c-9756-750e5e46df59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: matplotlib in /home/g2/.pyenv/versions/3.10.13/lib/python3.10/site-packages (3.10.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /home/g2/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /home/g2/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /home/g2/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /home/g2/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /home/g2/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from matplotlib) (2.2.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/g2/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /home/g2/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /home/g2/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /home/g2/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /home/g2/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Requirement already satisfied: ipywidgets in /home/g2/.pyenv/versions/3.10.13/lib/python3.10/site-packages (8.1.7)\n",
            "Requirement already satisfied: comm>=0.1.3 in /home/g2/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from ipywidgets) (0.2.2)\n",
            "Requirement already satisfied: ipython>=6.1.0 in /home/g2/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from ipywidgets) (8.36.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /home/g2/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from ipywidgets) (5.14.3)\n",
            "Requirement already satisfied: widgetsnbextension~=4.0.14 in /home/g2/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from ipywidgets) (4.0.14)\n",
            "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /home/g2/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from ipywidgets) (3.0.15)\n",
            "Requirement already satisfied: decorator in /home/g2/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
            "Requirement already satisfied: exceptiongroup in /home/g2/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (1.3.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /home/g2/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
            "Requirement already satisfied: matplotlib-inline in /home/g2/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /home/g2/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /home/g2/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
            "Requirement already satisfied: pygments>=2.4.0 in /home/g2/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
            "Requirement already satisfied: stack_data in /home/g2/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
            "Requirement already satisfied: typing_extensions>=4.6 in /home/g2/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.15.0)\n",
            "Requirement already satisfied: wcwidth in /home/g2/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/g2/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /home/g2/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: executing>=1.2.0 in /home/g2/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in /home/g2/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
            "Requirement already satisfied: pure-eval in /home/g2/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Requirement already satisfied: jupyterlab-widgets in /home/g2/.pyenv/versions/3.10.13/lib/python3.10/site-packages (3.0.15)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install matplotlib\n",
        "!pip install -q transformers torch accelerate bitsandbytes sentencepiece pillow pandas matplotlib tqdm\n",
        "!pip install ipywidgets\n",
        "!pip install jupyterlab-widgets\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dqg6mt3Nm5M0"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "import os # Added os import here for use in save_jsonl\n",
        "\n",
        "# A minimal dictionary to replace the one from utils.py\n",
        "sys_prompt = {\n",
        "    'blip2 style': \"Analyze the chart and answer the question: {}\",\n",
        "    'chartqa': \"Answer the following question based on the chart: {}\"\n",
        "}\n",
        "\n",
        "# A minimal base class to replace the one from utils.py\n",
        "class ChartBenchTester:\n",
        "    def __init__(self, test_index, sys_prompt_acc, sys_prompt_nqa, **kwargs):\n",
        "        self.test_index = test_index\n",
        "        self.system_prompt_acc = sys_prompt_acc\n",
        "        self.system_prompt_nqa = sys_prompt_nqa\n",
        "        self.image_root = ''\n",
        "        for key, value in kwargs.items():\n",
        "            setattr(self, key, value)\n",
        "\n",
        "    def reset_image_root(self, image_root):\n",
        "        self.image_root = image_root\n",
        "\n",
        "    def load_test_file(self, file_path, mode='r'):\n",
        "        \"\"\"Loads a JSON or JSONL file.\"\"\"\n",
        "        try:\n",
        "            with open(file_path, mode, encoding='utf-8') as f:\n",
        "                if file_path.endswith('.jsonl'):\n",
        "                    print(f\"Loading JSONL file: {file_path}\")\n",
        "                    return [json.loads(line) for line in f]\n",
        "                elif file_path.endswith('.json'):\n",
        "                    print(f\"Loading JSON file: {file_path}\")\n",
        "                    return json.load(f) # Assumes a standard JSON list\n",
        "                else:\n",
        "                    print(f\"Error: Unknown file type {file_path}. Must be .json or .jsonl\")\n",
        "                    return []\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Error: The file {file_path} was not found.\")\n",
        "            return []\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_path}: {e}\")\n",
        "            return []\n",
        "\n",
        "    def save_jsonl(self, file_path, data, mode='a+'):\n",
        "        \"\"\"Saves data to a JSONL file.\"\"\"\n",
        "        # Ensure the directory exists before writing\n",
        "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
        "        with open(file_path, mode, encoding='utf-8') as f:\n",
        "            for item in data:\n",
        "                f.write(json.dumps(item) + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Q-jXK3_bm8Aq"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'Cache' from 'transformers' (/home/g2/.pyenv/versions/3.10.13/lib/python3.10/site-packages/transformers/__init__.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01maccelerate\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpeft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PeftModel  \u001b[38;5;66;03m# <-- ADD THIS IMPORT\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01maccelerate\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m infer_auto_device_map, dispatch_model\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhuggingface_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m snapshot_download  \u001b[38;5;66;03m# <-- ADD THIS IMPORT\u001b[39;00m\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/peft/__init__.py:17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2023-present the HuggingFace Inc. team.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.17.1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     18\u001b[0m     MODEL_TYPE_TO_PEFT_MODEL_MAPPING,\n\u001b[1;32m     19\u001b[0m     AutoPeftModel,\n\u001b[1;32m     20\u001b[0m     AutoPeftModelForCausalLM,\n\u001b[1;32m     21\u001b[0m     AutoPeftModelForFeatureExtraction,\n\u001b[1;32m     22\u001b[0m     AutoPeftModelForQuestionAnswering,\n\u001b[1;32m     23\u001b[0m     AutoPeftModelForSeq2SeqLM,\n\u001b[1;32m     24\u001b[0m     AutoPeftModelForSequenceClassification,\n\u001b[1;32m     25\u001b[0m     AutoPeftModelForTokenClassification,\n\u001b[1;32m     26\u001b[0m )\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PeftConfig, PromptLearningConfig\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmapping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     29\u001b[0m     PEFT_TYPE_TO_CONFIG_MAPPING,\n\u001b[1;32m     30\u001b[0m     PEFT_TYPE_TO_MIXED_MODEL_MAPPING,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m     inject_adapter_in_model,\n\u001b[1;32m     34\u001b[0m )\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/peft/auto.py:32\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     22\u001b[0m     AutoModel,\n\u001b[1;32m     23\u001b[0m     AutoModelForCausalLM,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     AutoTokenizer,\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PeftConfig\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpeft_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     33\u001b[0m     PeftModel,\n\u001b[1;32m     34\u001b[0m     PeftModelForCausalLM,\n\u001b[1;32m     35\u001b[0m     PeftModelForFeatureExtraction,\n\u001b[1;32m     36\u001b[0m     PeftModelForQuestionAnswering,\n\u001b[1;32m     37\u001b[0m     PeftModelForSeq2SeqLM,\n\u001b[1;32m     38\u001b[0m     PeftModelForSequenceClassification,\n\u001b[1;32m     39\u001b[0m     PeftModelForTokenClassification,\n\u001b[1;32m     40\u001b[0m )\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TOKENIZER_CONFIG_NAME\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mother\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m check_file_exists_on_hf_hub\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/peft/peft_model.py:37\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msafetensors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m save_file \u001b[38;5;28;01mas\u001b[39;00m safe_save_file\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Cache, DynamicCache, EncoderDecoderCache, HybridCache, PreTrainedModel\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_outputs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m QuestionAnsweringModelOutput, SequenceClassifierOutput, TokenClassifierOutput\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PushToHubMixin\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'Cache' from 'transformers' (/home/g2/.pyenv/versions/3.10.13/lib/python3.10/site-packages/transformers/__init__.py)"
          ]
        }
      ],
      "source": [
        "import sys, copy\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import warnings\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "import torch\n",
        "import accelerate\n",
        "from peft import PeftModel  # <-- ADD THIS IMPORT\n",
        "from accelerate import infer_auto_device_map, dispatch_model\n",
        "from huggingface_hub import snapshot_download  # <-- ADD THIS IMPORT\n",
        "from transformers import (\n",
        "    AutoProcessor, Pix2StructForConditionalGeneration, \n",
        "    AutoModelForVision2Seq, AutoConfig\n",
        ")\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fShmxCXAm9ce"
      },
      "outputs": [],
      "source": [
        "# --- CONSTANTS ---\n",
        "\n",
        "# DePlot model (using the fixed Hugging Face Hub ID to avoid local path errors)\n",
        "BASE_MODEL_PATH = \"Qwen/Qwen2-VL-7B-Instruct\"\n",
        "ADAPTER_PATH = \"sergiopaniego/qwen2-7b-instruct-ChartQA\" \n",
        "CKPT_PATH = \"google/deplot\"\n",
        "\n",
        "# CKPT_PATH = \"google/deplot\" (you already have this)\n",
        "IMG_ROOT = \"/home/g2/ChartQA Dataset/test/png\"\n",
        "\n",
        "# --- MODIFIED: Paths for both test files ---\n",
        "TEST_AUGMENTED_FILE = \"/home/g2/ChartQA Dataset/test/test_augmented.json\"\n",
        "TEST_HUMAN_FILE = \"/home/g2/ChartQA Dataset/test/test_human.json\"\n",
        "\n",
        "# --- MODIFIED: Output file paths for each test ---\n",
        "# Base path for saving results\n",
        "SAVE_DIR = \"/home/g2/Chart Classifier\"\n",
        "# MODIFIED: Renamed output files to reflect Pix2Struct-VQA\n",
        "SAVE_PATH_AUGMENTED = os.path.join(SAVE_DIR, \"qwen7b_augmented.jsonl\")\n",
        "SAVE_PATH_HUMAN = os.path.join(SAVE_DIR, \"qwen7b_human.jsonl\")\n",
        "\n",
        "# --- ADDED: Intermediate DePlot Output Paths ---\n",
        "DEPLOT_OUTPUT_AUGMENTED = \"deplot_augmented_output.jsonl\"\n",
        "DEPLOT_OUTPUT_HUMAN = \"deplot_human_output.jsonl\"\n",
        "\n",
        "# --- ADDED: Final Qwen Output Paths (Aliasing existing SAVE_PATH) ---\n",
        "# QWEN_OUTPUT_HUMAN is the final save path, which should match the existing constant.\n",
        "QWEN_OUTPUT_AUGMENTED = SAVE_PATH_AUGMENTED\n",
        "QWEN_OUTPUT_HUMAN = SAVE_PATH_HUMAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fG3Y8g42nAV1"
      },
      "outputs": [],
      "source": [
        "class QwenChartBenchTester(ChartBenchTester):\n",
        "    \"\"\"\n",
        "    This class now runs in two stages:\n",
        "    1. run_deplot_stage: Generates data tables from images.\n",
        "    2. run_qwen_stage: Runs Base model (CoT) with a math-solver step.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- Part 1: Model Loading & Inference ---\n",
        "\n",
        "    def load_model(self):\n",
        "        \"\"\"Loads DePlot, the Qwen-VL base model, and the ChartQA adapter.\"\"\"\n",
        "        print(\"Loading DePlot model...\")\n",
        "        #self.deplot_processor = AutoProcessor.from_pretrained(CKPT_PATH)\n",
        "        #self.deplot_model = Pix2StructForConditionalGeneration.from_pretrained(CKPT_PATH).to(\"cuda\")\n",
        "        print(\"DePlot model loaded.\")\n",
        "\n",
        "        print(\"Loading Qwen-VL model...\")\n",
        "        \n",
        "        self.qwen_processor = AutoProcessor.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True)\n",
        "        \n",
        "        offload_folder = \"./qwen_offload\"\n",
        "        os.makedirs(offload_folder, exist_ok=True) \n",
        "\n",
        "        # 1. Load the Base Model onto the 'meta' device\n",
        "        print(f\"Loading base model {BASE_MODEL_PATH} onto meta device...\")\n",
        "        # low_cpu_mem_usage=True loads the model structure and weights \n",
        "        # onto 'meta' without using CPU RAM.\n",
        "        self.qwen_model = AutoModelForVision2Seq.from_pretrained(\n",
        "            BASE_MODEL_PATH,\n",
        "            torch_dtype=torch.float16,\n",
        "            trust_remote_code=True,\n",
        "            low_cpu_mem_usage=True # <-- This is the key\n",
        "            # No device_map or offload_dir here\n",
        "        )\n",
        "        print(\"Base model loaded on meta device.\")\n",
        "\n",
        "        # 2. Manually create the device map\n",
        "        print(\"Inferring device map...\")\n",
        "        no_split_modules = self.qwen_model._no_split_modules\n",
        "        device_map = infer_auto_device_map(\n",
        "            self.qwen_model,\n",
        "            max_memory={0: \"20GiB\", \"cpu\": \"30GiB\"}, # You can adjust these\n",
        "            no_split_module_classes=no_split_modules\n",
        "        )\n",
        "        print(f\"Device map inferred: {device_map}\")\n",
        "\n",
        "        # 3. Manually DISPATCH the model (don't need to load checkpoints)\n",
        "        print(\"Dispatching base model...\")\n",
        "        self.qwen_model = dispatch_model(\n",
        "            self.qwen_model,\n",
        "            device_map=device_map,\n",
        "            offload_dir=offload_folder\n",
        "        )\n",
        "        print(\"Base model dispatched.\")\n",
        "        \n",
        "        # 4. Load the Adapter\n",
        "        print(f\"Loading adapter from {ADAPTER_PATH}...\")\n",
        "        self.qwen_model.load_adapter(ADAPTER_PATH)\n",
        "        print(\"Adapter loaded.\")\n",
        "\n",
        "        self.qwen_model.eval() \n",
        "        print(\"Qwen-VL model + adapter ready.\")\n",
        "\n",
        "    def get_model_response(self, image: Image.Image, prompt_text: str) -> str:\n",
        "        \"\"\"Helper function to run inference with Qwen-VL.\"\"\"\n",
        "        messages = [\n",
        "            {\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": prompt_text}]}\n",
        "        ]\n",
        "        text = self.qwen_processor.apply_chat_template(\n",
        "            messages, tokenize=False, add_generation_prompt=True\n",
        "        )\n",
        "        inputs = self.qwen_processor(\n",
        "            text=text, images=[image], return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            output_ids = self.qwen_model.generate(**inputs, max_new_tokens=512, do_sample=False)\n",
        "\n",
        "        response_ids = output_ids[:, inputs.input_ids.shape[1]:]\n",
        "        response = self.qwen_processor.decode(response_ids[0], skip_special_tokens=True).strip()\n",
        "        return response\n",
        "\n",
        "    # --- ADDED: \"Math Solver\" Simulation ---\n",
        "    def _solve_math_expression(self, expression_str: str) -> str:\n",
        "        \"\"\"\n",
        "        Simulates a 'math solver small llm' by safely evaluating\n",
        "        a mathematical expression.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Sanitize: Allow only digits, dots, +, -, *, /, (, )\n",
        "            allowed_chars = \"0123456789.+-*/() \"\n",
        "            # Check if any character in the expression is NOT in the allowed list\n",
        "            if any(char not in allowed_chars for char in expression_str):\n",
        "                # Remove whitespace from the offending string for a cleaner error\n",
        "                offending_chars = \"\".join(set(c for c in expression_str if c not in allowed_chars))\n",
        "                return f\"[Error: Invalid characters in expression: {offending_chars}]\"\n",
        "\n",
        "            # WARNING: eval() is used here as a placeholder for a \"math solver\".\n",
        "            # In a real-world scenario, a safer parsing library should be used.\n",
        "            result = eval(expression_str)\n",
        "            return str(result)\n",
        "        except Exception as e:\n",
        "            return f\"[Error: Could not solve '{expression_str}'. Details: {e}]\"\n",
        "\n",
        "\n",
        "    # --- \"BASE MODEL\" (CoT) - MODIFIED for Math Solver ---\n",
        "    def run_cot(self, image: Image.Image, question: str) -> str:\n",
        "        \"\"\"\n",
        "        Runs the base Chain-of-Thought (CoT) model, with a simulated\n",
        "        call to a 'math solver llm' if a calculation is detected.\n",
        "        \"\"\"\n",
        "\n",
        "        # --- Step 1: Initial Reasoning Prompt ---\n",
        "        # Ask the model to reason and output a specific [CALCULATE: ...] tag\n",
        "        # if math is needed.\n",
        "        prompt_step_1 = f\"\"\"You are an expert chart analyst.\n",
        "Your task is to answer a question about the provided chart.\n",
        "Analyze the chart step-by-step.\n",
        "If you need to perform a mathematical calculation to answer the question,\n",
        "output ONLY the tag `[CALCULATE: expression]`.\n",
        "For example: `[CALCULATE: 100 - (25 + 30)]`.\n",
        "Do not solve it yourself.\n",
        "If no calculation is needed, or if you can answer directly, provide your\n",
        "full reasoning and then end with: \"The final answer is: [Your Answer]\".\n",
        "\n",
        "Question: {question}\n",
        "Let's think step by step:\n",
        "\"\"\"\n",
        "        initial_reasoning = self.get_model_response(image, prompt_step_1)\n",
        "\n",
        "        # --- Step 2: Check for Math Tag ---\n",
        "        # Use re.DOTALL to make '.' match newlines, in case the tag is split\n",
        "        calc_match = re.search(r\"\\[CALCULATE:\\s*(.*?)\\s*\\]\", initial_reasoning, re.IGNORECASE | re.DOTALL)\n",
        "\n",
        "        if calc_match:\n",
        "            # --- Step 3: Call \"Math Solver\" (Simulation) ---\n",
        "            expression_to_solve = calc_match.group(1).strip()\n",
        "            print(f\"  [run_cot] Detected math expression: {expression_to_solve}\")\n",
        "            math_result = self._solve_math_expression(expression_to_solve)\n",
        "            print(f\"  [run_cot] Math result: {math_result}\")\n",
        "\n",
        "            if \"[Error:\" in math_result:\n",
        "                # If math solver fails, return the error\n",
        "                return f\"Step 1 Reasoning: {initial_reasoning}\\nMath Solver Error: {math_result}\\nThe final answer is: [Math Error]\"\n",
        "\n",
        "            # --- Step 4: Final Answer Prompt ---\n",
        "            # Feed the result back to the model to get the final answer\n",
        "            prompt_step_2 = f\"\"\"You are an expert chart analyst.\n",
        "You were asked the following question about a chart:\n",
        "{question}\n",
        "\n",
        "Your initial analysis determined that a calculation was needed.\n",
        "Calculation: `{expression_to_solve}`\n",
        "Result: `{math_result}`\n",
        "\n",
        "Now, use this result to provide the final answer.\n",
        "Provide your reasoning and end with: \"The final answer is: [Your Answer]\".\n",
        "\n",
        "Reasoning using the calculation:\n",
        "\"\"\"\n",
        "            final_reasoning = self.get_model_response(image, prompt_step_2)\n",
        "\n",
        "            # Combine the reasoning for logging\n",
        "            combined_reasoning = f\"--- Step 1: Math Detection ---\\n{initial_reasoning}\\n\\n--- Step 2: Math Solver ---\\nInput: {expression_to_solve}\\nOutput: {math_result}\\n\\n--- Step 3: Final Answer ---\\n{final_reasoning}\"\n",
        "            return combined_reasoning\n",
        "        else:\n",
        "            # No calculation tag was found.\n",
        "            # Return the initial reasoning as-is.\n",
        "            return initial_reasoning\n",
        "\n",
        "    # --- GoT Methods (REMOVED) ---\n",
        "\n",
        "    def extract_final_answer(self, reasoning_text: str) -> str:\n",
        "        \"\"\"Extracts the final answer from the reasoning text.\"\"\"\n",
        "        match = re.search(r\"The final answer is:\\s*(.*)\", reasoning_text, re.IGNORECASE | re.DOTALL)\n",
        "        if match:\n",
        "            return match.group(1).strip()\n",
        "        else:\n",
        "            # Fallback: return the last non-empty line\n",
        "            lines = [line.strip() for line in reasoning_text.split('\\n') if line.strip()]\n",
        "            return lines[-1] if lines else \"\"\n",
        "\n",
        "    def calculate_relaxed_accuracy(self, predicted: str, ground_truth: str) -> bool:\n",
        "        \"\"\"Calculates relaxed accuracy (substring match).\"\"\"\n",
        "        pred_norm = re.sub(r\"[^\\w\\s\\.]\", \"\", str(predicted)).lower().strip()\n",
        "        gt_norm = re.sub(r\"[^\\w\\s\\.]\", \"\", str(ground_truth)).lower().strip()\n",
        "        if not pred_norm or not gt_norm:\n",
        "            return False\n",
        "        return (pred_norm in gt_norm) or (gt_norm in pred_norm)\n",
        "\n",
        "    # --- Part 2: Evaluation Stages ---\n",
        "\n",
        "    # ### --- STAGE 1: DePLOT EXECUTION --- ###\n",
        "    def run_deplot_stage(self, test_file_path, output_deplot_path):\n",
        "        \"\"\"\n",
        "        Runs ONLY the DePlot stage to generate data tables from images.\n",
        "        Saves results to an intermediate JSONL file.\n",
        "        \"\"\"\n",
        "        print(f\"--- Starting DePlot Stage for: {test_file_path} ---\")\n",
        "        test_data = self.load_test_file(test_file_path)\n",
        "        if not test_data:\n",
        "            return\n",
        "\n",
        "        ckpt_index = 0\n",
        "        if os.path.exists(output_deplot_path):\n",
        "            try:\n",
        "                ckpt_index = len(self.load_test_file(output_deplot_path, mode='r'))\n",
        "                if ckpt_index > 0:\n",
        "                    print(f\"DePlot progress file found. Resuming from sample {ckpt_index}...\")\n",
        "            except Exception as e:\n",
        "                print(f\"Could not read DePlot checkpoint, starting from 0. Error: {e}\")\n",
        "        else:\n",
        "            print(\"No DePlot progress file found. Starting from sample 0.\")\n",
        "\n",
        "        deplot_prompt = \"Generate the underlying data table for this chart:\"\n",
        "        deplot_task_prefix = \"<GRAPH_TO_TEXT>\"\n",
        "\n",
        "        for i in tqdm(range(ckpt_index, len(test_data)), desc=\"Stage 1: DePlot\"):\n",
        "            sample = test_data[i]\n",
        "            im_path = os.path.join(self.image_root, sample['imgname'])\n",
        "            result_entry = copy.deepcopy(sample)\n",
        "\n",
        "            try:\n",
        "                image = Image.open(im_path).convert('RGB')\n",
        "                inputs = self.deplot_processor(images=image, text=deplot_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "                with torch.inference_mode():\n",
        "                    generated_ids = self.deplot_model.generate(**inputs, max_new_tokens=1024)\n",
        "\n",
        "                generated_text = self.deplot_processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "                result_entry['deplot_table'] = generated_text.replace(deplot_task_prefix, \"\").strip()\n",
        "\n",
        "            except (KeyError, TypeError) as e:\n",
        "                print(f\"Skipping item {i}. Reason: Key error '{e}'. Check JSON format.\")\n",
        "            except FileNotFoundError:\n",
        "                print(f\"Skipping item {i}. Reason: Image file not found at {im_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"ERROR processing item {im_path} in DePlot stage: {e}\")\n",
        "\n",
        "            # Save progress immediately\n",
        "            self.save_jsonl(output_deplot_path, [result_entry], mode='a+')\n",
        "\n",
        "        print(f\"--- DePlot Stage Complete. Results saved to: {output_deplot_path} ---\")\n",
        "\n",
        "    # ### --- STAGE 2: QWEN EXECUTION (Unchanged) --- ###\n",
        "    # This method is unchanged, but its call to `self.run_cot`\n",
        "    # will now use the new math-solver logic.\n",
        "    def run_qwen_stage(self, deplot_file_path, output_qwen_path, run_name):\n",
        "        \"\"\"\n",
        "        Runs the Qwen (Base model) comparison using pre-computed DePlot results.\n",
        "        \"\"\"\n",
        "        print(f\"--- Starting Qwen Stage for: {run_name} ---\")\n",
        "        print(f\"Loading DePlot results from: {deplot_file_path}\")\n",
        "\n",
        "        deplot_results = self.load_test_file(deplot_file_path, mode='r')\n",
        "        if not deplot_results:\n",
        "            print(f\"No DePlot results found at {deplot_file_path}. Cannot run Qwen stage.\")\n",
        "            return\n",
        "\n",
        "        ckpt_index = 0\n",
        "        if os.path.exists(output_qwen_path):\n",
        "            try:\n",
        "                ckpt_index = len(self.load_test_file(output_qwen_path, mode='r'))\n",
        "                if ckpt_index > 0:\n",
        "                    print(f\"Qwen progress file found. Resuming from sample {ckpt_index}...\")\n",
        "            except Exception as e:\n",
        "                print(f\"Could not read Qwen checkpoint, starting from 0. Error: {e}\")\n",
        "        else:\n",
        "            print(\"No Qwen progress file found. Starting from sample 0.\")\n",
        "\n",
        "        hint = 'The key information in the chart has been extracted as below:\\n{}\\n'\n",
        "        all_results = []\n",
        "        if ckpt_index > 0:\n",
        "            print(\"Loading previous Qwen results for final summary...\")\n",
        "            all_results = self.load_test_file(output_qwen_path, mode='r')[:ckpt_index]\n",
        "\n",
        "        for i in tqdm(range(ckpt_index, len(deplot_results)), desc=\"Stage 2: Qwen (Base Model)\"):\n",
        "            sample = deplot_results[i]\n",
        "            im_path = os.path.join(self.image_root, sample['imgname'])\n",
        "\n",
        "            # Use a deepcopy to ensure original sample data is preserved\n",
        "            result_entry = copy.deepcopy(sample)\n",
        "\n",
        "            try:\n",
        "                question = sample['query']\n",
        "                ground_truth = str(sample['label'])\n",
        "                deplot_table = sample.get('deplot_table', '[Data table not available]') # Use .get for safety\n",
        "\n",
        "                question_with_hint = f\"{hint.format(deplot_table)}\\nQuestion: {question}\"\n",
        "\n",
        "                image = Image.open(im_path).convert('RGB')\n",
        "\n",
        "                # --- BASE MODEL (CoT) ---\n",
        "                # This call now invokes the new 2-step math logic\n",
        "                start_time = time.time()\n",
        "                base_reasoning = self.run_cot(image, question_with_hint)\n",
        "                base_time = time.time() - start_time\n",
        "                base_answer = self.extract_final_answer(base_reasoning)\n",
        "                base_correct = self.calculate_relaxed_accuracy(base_answer, ground_truth)\n",
        "\n",
        "                # --- GoT (REMOVED) ---\n",
        "\n",
        "                result_entry.update({\n",
        "                    \"base_reasoning\": base_reasoning, \"base_answer\": base_answer,\n",
        "                    \"base_time_s\": base_time, \"base_is_correct\": base_correct,\n",
        "                })\n",
        "\n",
        "            except FileNotFoundError:\n",
        "                print(f\"Skipping item {i}. Reason: Image file not found at {im_path}\")\n",
        "                continue\n",
        "            except Exception as e:\n",
        "                print(f\"ERROR processing item {im_path} in Qwen stage: {e}\")\n",
        "                result_entry.update({\"base_answer\": f\"Error: {e}\", \"base_reasoning\": f\"Error: {e}\"})\n",
        "\n",
        "            # Save Qwen progress immediately\n",
        "            self.save_jsonl(output_qwen_path, [result_entry], mode='a+')\n",
        "            all_results.append(result_entry)\n",
        "\n",
        "        # --- Print Final Summary ---\n",
        "        self._print_summary_for_type(run_name, all_results)\n",
        "\n",
        "    # ### --- Helper: Final Summary Printer (Unchanged) --- ###\n",
        "    def _print_summary_for_type(self, title, results_list):\n",
        "        \"\"\"Helper function to print a formatted summary.\"\"\"\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(f\"--- FINAL SUMMARY FOR: {title} ---\")\n",
        "\n",
        "        if not results_list:\n",
        "            print(\"No samples processed for this type.\")\n",
        "            print(\"=\"*50)\n",
        "            return\n",
        "\n",
        "        total_samples = len(results_list)\n",
        "\n",
        "        # Calculate Base metrics\n",
        "        base_correct_list = [r['base_is_correct'] for r in results_list if 'base_is_correct' in r]\n",
        "        base_time_list = [r['base_time_s'] for r in results_list if 'base_time_s' in r]\n",
        "\n",
        "        base_accuracy = np.mean(base_correct_list) * 100 if base_correct_list else 0.0\n",
        "        avg_base_time = np.mean(base_time_list) if base_time_list else 0.0\n",
        "\n",
        "        print(f\"Total samples processed: {total_samples}\")\n",
        "        print(\"-\" * 30)\n",
        "        print(f\"Base Model (CoT) Accuracy: {base_accuracy:.2f}%\")\n",
        "        print(f\"Average Base Model Time: {avg_base_time:.2f} s\")\n",
        "        print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381,
          "referenced_widgets": [
            "78205b8433dc4ebb9a4f001789acbbf2",
            "d864b5a0de2f48bd97761e76311dc0cf",
            "581d8a7cf29f41e78fb7a8752ca4f374",
            "cb66675c2f9f42cb9911c5461719a16a",
            "4628482007cb4dbcaf805bde30121a15",
            "73f46ec0c63b435d8ff7cd32e9ea4dcb",
            "8412a3a312544f3dbea4ed8597ec2dda",
            "a1a111e827bd4004908e2a5bb61b3042",
            "a99f35ecba8242f4b0563d94bc98462e",
            "1def66ea83e548bf96da3f0f4639ded8",
            "8357443019c241ae9bd23d0636c15936"
          ]
        },
        "id": "IcfqsGaInPWy",
        "outputId": "71f3b026-4ad0-48a4-a210-acf781c0dcf5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading DePlot model...\n",
            "DePlot model loaded.\n",
            "Loading Qwen-VL model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading base model Qwen/Qwen2-VL-7B-Instruct onto meta device...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b13d67d4dae942cc9821e617f3a7187d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e1a93fb6218b4792b25d2dc51b07c320",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def main():\n",
        "    # --- 1. Initialize Tester and Load Models ---\n",
        "    tester = QwenChartBenchTester(\n",
        "        test_index=None, # Will be set per-run\n",
        "        sys_prompt_acc=sys_prompt['blip2 style'],\n",
        "        sys_prompt_nqa=sys_prompt['chartqa']\n",
        "    )\n",
        "    tester.load_model()\n",
        "    tester.reset_image_root(IMG_ROOT)\n",
        "\n",
        "    # --- 2. Define the two test runs ---\n",
        "    test_runs = [\n",
        "        {\n",
        "            \"name\": \"Augmented Test\",\n",
        "            \"input_file\": TEST_AUGMENTED_FILE,\n",
        "            \"deplot_output_file\": DEPLOT_OUTPUT_AUGMENTED,\n",
        "            \"qwen_output_file\": QWEN_OUTPUT_AUGMENTED\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"Human Test\",\n",
        "            \"input_file\": TEST_HUMAN_FILE,\n",
        "            \"deplot_output_file\": DEPLOT_OUTPUT_HUMAN,\n",
        "            \"qwen_output_file\": QWEN_OUTPUT_HUMAN\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # # --- 3. Execute STAGE 1 (DePlot) for both ---\n",
        "    # print(\"\\n\" + \"#\"*70)\n",
        "    # print(\"### STARTING STAGE 1: DePLOT (Data Table Generation) ###\")\n",
        "    # print(\"#\"*70 + \"\\n\")\n",
        "    # for run in test_runs:\n",
        "    #     print(f\"--- Running DePlot for: {run['name']} ---\")\n",
        "    #     tester.test_index = run['input_file'] # Set the correct input\n",
        "    #     tester.run_deplot_stage(run['input_file'], run['deplot_output_file'])\n",
        "    #     print(f\"--- Finished DePlot for: {run['name']} ---\")\n",
        "\n",
        "    # --- 4. Execute STAGE 2 (Qwen) for both ---\n",
        "    print(\"\\n\" + \"#\"*70)\n",
        "    print(\"### STARTING STAGE 2: Qwen (Base Model) ###\")\n",
        "    print(\"#\"*70 + \"\\n\")\n",
        "    for run in test_runs:\n",
        "        print(f\"--- Running Qwen for: {run['name']} ---\")\n",
        "        # Pass the intermediate DePlot file, the final save path, and the run name\n",
        "        tester.run_qwen_stage(run['deplot_output_file'], run['qwen_output_file'], run['name'])\n",
        "        print(f\"\\n### FINISHED RUN: {run['name']} ###\")\n",
        "\n",
        "    print(\"\\nAll evaluations finished.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "3.10.13",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1def66ea83e548bf96da3f0f4639ded8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4628482007cb4dbcaf805bde30121a15": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "581d8a7cf29f41e78fb7a8752ca4f374": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1a111e827bd4004908e2a5bb61b3042",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a99f35ecba8242f4b0563d94bc98462e",
            "value": 2
          }
        },
        "73f46ec0c63b435d8ff7cd32e9ea4dcb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78205b8433dc4ebb9a4f001789acbbf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d864b5a0de2f48bd97761e76311dc0cf",
              "IPY_MODEL_581d8a7cf29f41e78fb7a8752ca4f374",
              "IPY_MODEL_cb66675c2f9f42cb9911c5461719a16a"
            ],
            "layout": "IPY_MODEL_4628482007cb4dbcaf805bde30121a15"
          }
        },
        "8357443019c241ae9bd23d0636c15936": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8412a3a312544f3dbea4ed8597ec2dda": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a1a111e827bd4004908e2a5bb61b3042": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a99f35ecba8242f4b0563d94bc98462e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cb66675c2f9f42cb9911c5461719a16a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1def66ea83e548bf96da3f0f4639ded8",
            "placeholder": "",
            "style": "IPY_MODEL_8357443019c241ae9bd23d0636c15936",
            "value": "2/2[00:29&lt;00:00,14.54s/it]"
          }
        },
        "d864b5a0de2f48bd97761e76311dc0cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73f46ec0c63b435d8ff7cd32e9ea4dcb",
            "placeholder": "",
            "style": "IPY_MODEL_8412a3a312544f3dbea4ed8597ec2dda",
            "value": "Loadingcheckpointshards:100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
